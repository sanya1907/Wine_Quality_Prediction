{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Using Random_Forest_Regressor\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data = pd.read_csv('/content/WineQT.csv')\n",
        "\n",
        "data.head(5)\n",
        "\n",
        "data = data.drop(columns=['Id'])\n",
        "\n",
        "data.head(5)\n",
        "\n",
        "X = data.iloc[:, :-1]\n",
        "y = data.iloc[:, -1]\n",
        "\n",
        "print(X)\n",
        "\n",
        "print(y)\n",
        "\n",
        "data.isnull().sum()\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "regressor = DecisionTreeRegressor(random_state = 0)\n",
        "regressor.fit(X, y)\n",
        "\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "y_pred\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "# Calculate evaluation metrics\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Display the evaluation metrics\n",
        "print(\"Mean Absolute Error (MAE):\", mae)\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
        "print(\"R-squared (R²) Score:\", r2)\n",
        "\n",
        "# Compare predictions with actual values\n",
        "print(\"\\nPredictions:\", y_pred)\n",
        "print(\"Actual values:\", y_test)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
        "plt.figure(figsize=(20,10))  # Set the figure size\n",
        "plot_tree(regressor, feature_names=X.columns, filled=True)\n",
        "plt.title('Decision Tree Regression Visualization')\n",
        "plt.show()\n",
        "\n",
        "importances = regressor.feature_importances_\n",
        "\n",
        "# Step 5: Identify the most important feature\n",
        "most_important_feature_index = np.argmax(importances)\n",
        "most_important_feature_name = X.columns[most_important_feature_index]\n",
        "\n",
        "print(f\"The most important feature is: {most_important_feature_name} with importance {importances[most_important_feature_index]}\")"
      ],
      "metadata": {
        "id": "On7zPCE-5dDk",
        "outputId": "0cf8bebe-8c20-46b2-bcf3-0095b6eabd22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
            "0               7.4             0.700         0.00             1.9      0.076   \n",
            "1               7.8             0.880         0.00             2.6      0.098   \n",
            "2               7.8             0.760         0.04             2.3      0.092   \n",
            "3              11.2             0.280         0.56             1.9      0.075   \n",
            "4               7.4             0.700         0.00             1.9      0.076   \n",
            "...             ...               ...          ...             ...        ...   \n",
            "1138            6.3             0.510         0.13             2.3      0.076   \n",
            "1139            6.8             0.620         0.08             1.9      0.068   \n",
            "1140            6.2             0.600         0.08             2.0      0.090   \n",
            "1141            5.9             0.550         0.10             2.2      0.062   \n",
            "1142            5.9             0.645         0.12             2.0      0.075   \n",
            "\n",
            "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
            "0                    11.0                  34.0  0.99780  3.51       0.56   \n",
            "1                    25.0                  67.0  0.99680  3.20       0.68   \n",
            "2                    15.0                  54.0  0.99700  3.26       0.65   \n",
            "3                    17.0                  60.0  0.99800  3.16       0.58   \n",
            "4                    11.0                  34.0  0.99780  3.51       0.56   \n",
            "...                   ...                   ...      ...   ...        ...   \n",
            "1138                 29.0                  40.0  0.99574  3.42       0.75   \n",
            "1139                 28.0                  38.0  0.99651  3.42       0.82   \n",
            "1140                 32.0                  44.0  0.99490  3.45       0.58   \n",
            "1141                 39.0                  51.0  0.99512  3.52       0.76   \n",
            "1142                 32.0                  44.0  0.99547  3.57       0.71   \n",
            "\n",
            "      alcohol  \n",
            "0         9.4  \n",
            "1         9.8  \n",
            "2         9.8  \n",
            "3         9.8  \n",
            "4         9.4  \n",
            "...       ...  \n",
            "1138     11.0  \n",
            "1139      9.5  \n",
            "1140     10.5  \n",
            "1141     11.2  \n",
            "1142     10.2  \n",
            "\n",
            "[1143 rows x 11 columns]\n",
            "0       5\n",
            "1       5\n",
            "2       5\n",
            "3       6\n",
            "4       5\n",
            "       ..\n",
            "1138    6\n",
            "1139    6\n",
            "1140    5\n",
            "1141    6\n",
            "1142    5\n",
            "Name: quality, Length: 1143, dtype: int64\n",
            "Mean Absolute Error (MAE): 0.0\n",
            "Mean Squared Error (MSE): 0.0\n",
            "Root Mean Squared Error (RMSE): 0.0\n",
            "R-squared (R²) Score: 1.0\n",
            "\n",
            "Predictions: [5. 6. 5. 6. 6. 8. 5. 5. 6. 5. 7. 6. 6. 6. 5. 6. 5. 5. 5. 6. 6. 6. 5. 7.\n",
            " 6. 5. 7. 6. 5. 6. 6. 6. 6. 5. 6. 5. 6. 7. 6. 5. 7. 5. 6. 5. 4. 5. 6. 6.\n",
            " 5. 6. 7. 5. 6. 7. 5. 7. 6. 4. 6. 5. 5. 6. 5. 6. 6. 6. 6. 5. 5. 5. 6. 5.\n",
            " 6. 6. 5. 5. 5. 5. 5. 6. 5. 6. 5. 6. 6. 6. 6. 5. 6. 6. 5. 5. 6. 5. 5. 5.\n",
            " 6. 7. 6. 6. 7. 6. 5. 6. 5. 5. 6. 7. 6. 7. 7. 5. 5. 5. 6. 5. 5. 8. 6. 5.\n",
            " 5. 5. 5. 6. 5. 6. 6. 6. 6. 6. 5. 5. 7. 6. 5. 5. 5. 6. 5. 6. 7. 6. 6. 5.\n",
            " 7. 5. 5. 5. 5. 5. 7. 6. 6. 5. 6. 6. 5. 5. 7. 5. 5. 5. 6. 5. 7. 5. 6. 4.\n",
            " 6. 5. 6. 5. 5. 6. 5. 5. 7. 5. 6. 6. 6. 5. 6. 6. 5. 4. 6. 5. 6. 4. 6. 6.\n",
            " 7. 6. 5. 6. 5. 7. 7. 6. 6. 6. 5. 5. 7. 7. 5. 5. 6. 6. 5. 6. 6. 6. 5. 6.\n",
            " 7. 5. 6. 6. 6. 5. 5. 5. 4. 6. 5. 6. 6.]\n",
            "Actual values: 158     5\n",
            "1081    6\n",
            "291     5\n",
            "538     6\n",
            "367     6\n",
            "       ..\n",
            "66      4\n",
            "328     6\n",
            "67      5\n",
            "231     6\n",
            "966     6\n",
            "Name: quality, Length: 229, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Decision_Tree_Regressor\n",
        "#Using Random_Forest_Regressor\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data = pd.read_csv('/content/WineQT.csv')\n",
        "\n",
        "data.head(5)\n",
        "\n",
        "data = data.drop(columns=['Id'])\n",
        "\n",
        "data.head(5)\n",
        "\n",
        "X = data.iloc[:, :-1]\n",
        "y = data.iloc[:, -1]\n",
        "\n",
        "print(X)\n",
        "\n",
        "print(y)\n",
        "\n",
        "data.isnull().sum()\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "regressor = RandomForestRegressor(n_estimators = 10, random_state = 0)\n",
        "regressor.fit(X, y)\n",
        "\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "y_pred\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "# Calculate evaluation metrics\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Display the evaluation metrics\n",
        "print(\"Mean Absolute Error (MAE):\", mae)\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
        "print(\"R-squared (R²) Score:\", r2)\n",
        "\n",
        "# Compare predictions with actual values\n",
        "print(\"\\nPredictions:\", y_pred)\n",
        "print(\"Actual values:\", y_test)\n",
        "\n",
        "\n",
        "importances = regressor.feature_importances_\n",
        "\n",
        "# Step 5: Identify the most important feature\n",
        "most_important_feature_index = np.argmax(importances)\n",
        "most_important_feature_name = X.columns[most_important_feature_index]\n",
        "\n",
        "print(f\"The most important feature is: {most_important_feature_name} with importance {importances[most_important_feature_index]}\")"
      ],
      "metadata": {
        "id": "im3rC5BA5df6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4edde6c-d433-4997-9501-3be46914acba"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
            "0               7.4             0.700         0.00             1.9      0.076   \n",
            "1               7.8             0.880         0.00             2.6      0.098   \n",
            "2               7.8             0.760         0.04             2.3      0.092   \n",
            "3              11.2             0.280         0.56             1.9      0.075   \n",
            "4               7.4             0.700         0.00             1.9      0.076   \n",
            "...             ...               ...          ...             ...        ...   \n",
            "1138            6.3             0.510         0.13             2.3      0.076   \n",
            "1139            6.8             0.620         0.08             1.9      0.068   \n",
            "1140            6.2             0.600         0.08             2.0      0.090   \n",
            "1141            5.9             0.550         0.10             2.2      0.062   \n",
            "1142            5.9             0.645         0.12             2.0      0.075   \n",
            "\n",
            "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
            "0                    11.0                  34.0  0.99780  3.51       0.56   \n",
            "1                    25.0                  67.0  0.99680  3.20       0.68   \n",
            "2                    15.0                  54.0  0.99700  3.26       0.65   \n",
            "3                    17.0                  60.0  0.99800  3.16       0.58   \n",
            "4                    11.0                  34.0  0.99780  3.51       0.56   \n",
            "...                   ...                   ...      ...   ...        ...   \n",
            "1138                 29.0                  40.0  0.99574  3.42       0.75   \n",
            "1139                 28.0                  38.0  0.99651  3.42       0.82   \n",
            "1140                 32.0                  44.0  0.99490  3.45       0.58   \n",
            "1141                 39.0                  51.0  0.99512  3.52       0.76   \n",
            "1142                 32.0                  44.0  0.99547  3.57       0.71   \n",
            "\n",
            "      alcohol  \n",
            "0         9.4  \n",
            "1         9.8  \n",
            "2         9.8  \n",
            "3         9.8  \n",
            "4         9.4  \n",
            "...       ...  \n",
            "1138     11.0  \n",
            "1139      9.5  \n",
            "1140     10.5  \n",
            "1141     11.2  \n",
            "1142     10.2  \n",
            "\n",
            "[1143 rows x 11 columns]\n",
            "0       5\n",
            "1       5\n",
            "2       5\n",
            "3       6\n",
            "4       5\n",
            "       ..\n",
            "1138    6\n",
            "1139    6\n",
            "1140    5\n",
            "1141    6\n",
            "1142    5\n",
            "Name: quality, Length: 1143, dtype: int64\n",
            "Mean Absolute Error (MAE): 0.1772925764192139\n",
            "Mean Squared Error (MSE): 0.06524017467248908\n",
            "Root Mean Squared Error (RMSE): 0.25542156266159105\n",
            "R-squared (R²) Score: 0.8827612912069085\n",
            "\n",
            "Predictions: [5.2 5.9 5.2 5.5 6.2 7.4 5.1 5.  5.8 5.2 7.  6.1 5.9 6.1 5.2 5.8 5.1 5.1\n",
            " 5.  6.6 6.  5.9 5.  6.9 6.  5.1 6.7 6.1 5.  5.8 5.8 5.6 6.1 5.3 5.9 5.1\n",
            " 6.  7.1 6.2 5.2 6.9 5.4 6.  5.3 4.7 4.9 5.9 5.9 5.  6.  6.9 5.2 5.6 6.9\n",
            " 5.  6.9 5.9 4.8 5.9 5.  5.  5.9 5.5 6.1 6.3 5.9 6.4 5.  5.2 5.3 6.5 5.1\n",
            " 5.5 6.  4.9 5.  5.1 5.6 5.4 5.8 5.2 6.  5.4 5.9 5.8 6.1 5.7 5.2 6.  6.\n",
            " 4.9 5.7 5.9 5.2 4.9 5.  5.8 6.7 5.9 5.6 6.4 6.2 5.2 6.1 5.2 5.1 6.1 6.5\n",
            " 5.9 7.  7.  4.9 5.  5.7 5.7 5.3 5.6 7.5 5.8 5.2 5.3 5.1 5.  5.9 5.4 5.9\n",
            " 5.8 6.1 5.8 6.  5.  5.  7.  5.9 6.  5.  4.9 5.5 5.  6.  7.  6.  6.  5.\n",
            " 6.7 5.  5.  5.1 5.  5.  6.7 5.7 6.  5.2 6.1 5.8 5.1 5.2 6.9 5.1 5.1 5.2\n",
            " 6.  5.4 6.8 5.  6.4 4.9 6.  5.  5.9 5.1 5.2 5.6 5.1 5.1 6.7 5.2 6.1 5.6\n",
            " 6.4 5.3 6.2 5.8 5.  4.4 6.2 5.  6.2 4.6 6.  6.3 6.9 5.7 5.4 6.  5.1 7.\n",
            " 7.  6.3 5.8 6.  5.1 5.2 6.6 6.9 5.  5.  6.4 6.  5.4 6.3 5.9 6.  5.  5.8\n",
            " 6.7 5.  6.1 5.9 5.9 5.2 5.1 4.9 4.1 6.  5.3 5.9 6. ]\n",
            "Actual values: 158     5\n",
            "1081    6\n",
            "291     5\n",
            "538     6\n",
            "367     6\n",
            "       ..\n",
            "66      4\n",
            "328     6\n",
            "67      5\n",
            "231     6\n",
            "966     6\n",
            "Name: quality, Length: 229, dtype: int64\n",
            "The most important feature is: alcohol with importance 0.29741861935919417\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above two codes has been done to show difference between the performance of DecisionTreeRegressor and RandomForestRegressor.\n",
        "THe result was that DecisionTreeRegressor is more efficient in giving the optimised result. The R^2 value of Decision_tree_regressor was founf to be 1.0 that proves the perfect fit"
      ],
      "metadata": {
        "id": "zswg5Y_K6Z-g"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xTfK3Knf6IT1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}